{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_segment(match_regex, tokens):\n",
    "    \"\"\"\n",
    "    Splits a sequence of tokens into sentences, splitting wherever the given matching regular expression\n",
    "    matches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens      the input sequence as list of strings (each item is a ``word'')\n",
    "    match_regex the regular expression that defines at which token to split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a list of lists of strings, where each string is a word, and each inner list\n",
    "    represents a sentence.\n",
    "    \"\"\"\n",
    "    sentences = [[]]\n",
    "    for tok in tokens:\n",
    "        sentences[-1].append(tok)\n",
    "        if match_regex.match(tok):\n",
    "            sentences.append([])\n",
    "            \n",
    "    if sentences[-1] == []:\n",
    "        del sentences[-1]\n",
    "    return sentences\n",
    "\n",
    "def load_entity_dict(file_path):\n",
    "    entity_dict = {}\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) >= 3:  \n",
    "                entity_name = parts[0]\n",
    "                entity_type = parts[2] \n",
    "                entity_dict[entity_name] = entity_type\n",
    "    return entity_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    token = re.compile(r'(?:[a-zA-Z]\\.){2,}|http://www\\.\\w+\\.\\w+|[\\w\\']+|[0-9]+\\.[0-9]+|[0-9]+|[.?!\",%-\\(\\)-]+')\n",
    "\n",
    "    open_text = open(text, 'r', encoding='utf-8')\n",
    "    raw_text = open_text.readlines()[4:]\n",
    "    raw_text = ' '.join(raw_text)\n",
    "    #print(raw_text)\n",
    "\n",
    "    tokens = token.findall(raw_text)\n",
    "    #print(tokens)\n",
    "    sentences = sentence_segment(re.compile(r'[\\.\\?\\!]'), tokens)\n",
    "    #for sentence in sentences:\n",
    "        #print(sentence)\n",
    "    open_text.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def tag_my_data(text, ent_dict):\n",
    "    #text=[['Саджид', 'Джавид', 'новоназначеният', 'вътрешен', 'министър', 'след', 'оставката', 'на', 'Ръд', 'обеща', 'да', 'осигури', 'отношение', 'на', 'порядъчност', 'и', 'честност', 'към', 'хората', 'засегнати', 'от', 'скандала', 'Уиндръш', '.']]\n",
    "    data=[]\n",
    "    for sentence in text: \n",
    "\n",
    "        tags = [\"O\"] * len(sentence)  # Default to \"O\" (Outside)\n",
    "    \n",
    "        for entity, tag in ent_dict.items():\n",
    "            entity_tokens = entity.split()\n",
    "            entity_length = len(entity_tokens)\n",
    "\n",
    "            for i in range(len(sentence) - entity_length + 1):\n",
    "                if sentence[i:i+entity_length] == entity_tokens:\n",
    "                    tags[i] = f\"B-{tag}\"  # Begin entity\n",
    "                    for j in range(1, entity_length):\n",
    "                        tags[i + j] = f\"I-{tag}\"  # Inside entity\n",
    "        data.append(list(zip(sentence, tags)))\n",
    "    return data\n",
    "\n",
    "def to_conll(doc_id, data):\n",
    "    output = []\n",
    "    sent_count = 1\n",
    "    for sentence in data:\n",
    "        words = [word for word, tag in sentence]\n",
    "        original_sent=\" \".join(words)\n",
    "        sent_id = f\"{doc_id}-{sent_count}\"\n",
    "        output.append(f\"# sent_id = {sent_id}\")\n",
    "        output.append(f\"# text = {original_sent}\")\n",
    "\n",
    "        for word, tag in sentence:\n",
    "\n",
    "            output.append(f\"{word}\\t{tag}\")\n",
    "        output.append(\"\")\n",
    "                \n",
    "        output.append(\"\")  # Blank line separates sentences\n",
    "        sent_count += 1\n",
    "\n",
    "    return \"\\n\".join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"brexit_bg.txt_file_1061.txt\"\n",
    "\n",
    "def aggregate(doc_id, text, entity_dict):\n",
    "    data=tokenize(text)\n",
    "    bio_output = tag_my_data(data, entity_dict)\n",
    "    dataset=to_conll(doc_id, bio_output)\n",
    "    return dataset\n",
    "\n",
    "#print(\"BIO-tagged data saved to 'manual.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO-tagged data saved to 'data/train/bg.txt'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataset=\"\"\n",
    "\n",
    "for file in os.listdir(\"data/bg/raw\"):\n",
    "    doc_id = \"bg_\" +  re.findall(r'\\d+', file)[0]  # Dummy doc ID, change as needed\n",
    "    dataset+=f\"# newdoc id = {doc_id}\"\n",
    "    if file.endswith(\".txt\"):\n",
    "        text=\"data/bg/raw/\"+file\n",
    "        labels=file[:-4]+\".out\"\n",
    "        entity_dict = load_entity_dict(\"data/bg/annotated/\"+labels)\n",
    "        #print(file, \" \", text, \" \", labels)\n",
    "        constructed_file=aggregate(doc_id, text, entity_dict)\n",
    "        dataset+=\"\\n\"+constructed_file\n",
    "    \n",
    "    \n",
    "\n",
    "with open(\"data/train/bg.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(dataset)\n",
    "\n",
    "print(\"BIO-tagged data saved to 'data/train/bg.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO-tagged data saved to 'data/train/uk.txt'.\n"
     ]
    }
   ],
   "source": [
    "dataset=\"\"\n",
    "\n",
    "for file in os.listdir(\"data/uk/raw\"):\n",
    "    doc_id = \"uk_\" +  re.findall(r'\\d+', file)[0]  # Dummy doc ID, change as needed\n",
    "    dataset+=f\"# newdoc id = {doc_id}\"\n",
    "    if file.endswith(\".txt\"):\n",
    "        text=\"data/uk/raw/\"+file\n",
    "        labels=file[:-4]+\".out\"\n",
    "        entity_dict = load_entity_dict(\"data/uk/annotated/\"+labels)\n",
    "        constructed_file=aggregate(doc_id, text, entity_dict)\n",
    "        dataset+=\"\\n\"+constructed_file\n",
    "    \n",
    "    \n",
    "\n",
    "with open(\"data/train/uk.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(dataset)\n",
    "\n",
    "print(\"BIO-tagged data saved to 'data/train/uk.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO-tagged data saved to 'data/train/sl.txt'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset=\"\"\n",
    "\n",
    "for file in os.listdir(\"data/sl/raw\"):\n",
    "    doc_id = \"sl_\" +  re.findall(r'\\d+', file)[0]  # Dummy doc ID, change as needed\n",
    "    dataset+=f\"# newdoc id = {doc_id}\"\n",
    "    if file.endswith(\".txt\"):\n",
    "        text=\"data/sl/raw/\"+file\n",
    "        labels=file[:-4]+\".out\"\n",
    "        entity_dict = load_entity_dict(\"data/sl/annotated/\"+labels)\n",
    "        constructed_file=aggregate(doc_id, text, entity_dict)\n",
    "        dataset+=\"\\n\"+constructed_file\n",
    "    \n",
    "    \n",
    "\n",
    "with open(\"data/train/sl.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(dataset)\n",
    "\n",
    "print(\"BIO-tagged data saved to 'data/train/sl.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO-tagged data saved to 'data/train/pl.txt'.\n"
     ]
    }
   ],
   "source": [
    "dataset=\"\"\n",
    "\n",
    "for file in os.listdir(\"data/pl/raw\"):\n",
    "    doc_id = \"pl_\" +  re.findall(r'\\d+', file)[0]  # Dummy doc ID, change as needed\n",
    "    dataset+=f\"# newdoc id = {doc_id}\"\n",
    "    if file.endswith(\".txt\"):\n",
    "        text=\"data/pl/raw/\"+file\n",
    "        labels=file[:-4]+\".out\"\n",
    "        entity_dict = load_entity_dict(\"data/pl/annotated/\"+labels)\n",
    "        constructed_file=aggregate(doc_id, text, entity_dict)\n",
    "        dataset+=\"\\n\"+constructed_file\n",
    "    \n",
    "    \n",
    "\n",
    "with open(\"data/train/pl.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(dataset)\n",
    "\n",
    "print(\"BIO-tagged data saved to 'data/train/pl.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO-tagged data saved to 'data/train/cs.txt'.\n"
     ]
    }
   ],
   "source": [
    "dataset=\"\"\n",
    "\n",
    "for file in os.listdir(\"data/cs/raw\"):\n",
    "    doc_id = \"cs_\" +  re.findall(r'\\d+', file)[0]  # Dummy doc ID, change as needed\n",
    "    dataset+=f\"# newdoc id = {doc_id}\"\n",
    "    if file.endswith(\".txt\"):\n",
    "        text=\"data/cs/raw/\"+file\n",
    "        labels=file[:-4]+\".out\"\n",
    "        entity_dict = load_entity_dict(\"data/cs/annotated/\"+labels)\n",
    "        constructed_file=aggregate(doc_id, text, entity_dict)\n",
    "        dataset+=\"\\n\"+constructed_file\n",
    "    \n",
    "    \n",
    "\n",
    "with open(\"data/train/cs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(dataset)\n",
    "\n",
    "print(\"BIO-tagged data saved to 'data/train/cs.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO-tagged data saved to 'data/train/ru.txt'.\n"
     ]
    }
   ],
   "source": [
    "dataset=\"\"\n",
    "\n",
    "for file in os.listdir(\"data/ru/raw\"):\n",
    "    doc_id = \"ru_\" +  re.findall(r'\\d+', file)[0]  # Dummy doc ID, change as needed\n",
    "    dataset+=f\"# newdoc id = {doc_id}\"\n",
    "    if file.endswith(\".txt\"):\n",
    "        text=\"data/ru/raw/\"+file\n",
    "        labels=file[:-4]+\".out\"\n",
    "        entity_dict = load_entity_dict(\"data/ru/annotated/\"+labels)\n",
    "        constructed_file=aggregate(doc_id, text, entity_dict)\n",
    "        dataset+=\"\\n\"+constructed_file\n",
    "    \n",
    "    \n",
    "\n",
    "with open(\"data/train/ru.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(dataset)\n",
    "\n",
    "print(\"BIO-tagged data saved to 'data/train/ru.txt'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "websoup",
   "language": "python",
   "name": "websoup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
